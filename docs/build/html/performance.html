

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Performance &mdash; MARVELsim 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <link rel="stylesheet" href="_static/fonts.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Extra examples" href="extra.html" />
    <link rel="prev" title="Tutorial" href="tutorial.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MARVELsim
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#vsc-information">VSC information</a></li>
<li class="toctree-l2"><a class="reference internal" href="#job-script-calibration-mode">Job script - Calibration mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#job-script-science-mode">Job script - Science mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#workflow-science-mode">Workflow - Science mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#workflows">Workflows</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extra.html">Extra examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgements.html">Acknowledgements</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MARVELsim</a>
        
      </nav>


      <div class="wy-nav-content">
<div class="git-ribbon">
  <a href="http://github.com/SwissDataScienceCenter" rel="me">Join us on GitHub</a>
</div>

        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Performance</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/performance.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="performance">
<h1>Performance<a class="headerlink" href="#performance" title="Permalink to this headline">¶</a></h1>
<p>In order to speed up the simulations we here demonstrate how to run MARVELsim on a High Performace Computing (HPC) facility. Generally, since PyEchelle is the natural bottleneck w.r.t. computation time, MARVELsim takes advantage of PyEchelle’s user friendly interface to run simulations on GPUs or normal CPUs. As this dramatically decrease the run time, we strongly recommend to checkout <a class="reference external" href="https://stuermer.gitlab.io/pyechelle/benchmark.html">PyEchelle’s documentation on performance</a> for a deeper understanding of what’s going on under the hood in what follows.</p>
<div class="admonition-step-by-step-guide admonition">
<p class="admonition-title">Step-by-step guide</p>
<p>We will use the <a class="reference external" href="https://www.vscentrum.be/getaccess">Vlaams Supercomputer Centrum (VSC)</a> as example on how run the calibration and science mode of MARVELsim. Follow these steps to run simulations on the VSC:</p>
<ul class="simple">
<li><p>If using science mode, generate a RV time series using <code class="docutils literal notranslate"><span class="pre">rv-generator.py</span></code></p></li>
<li><p>If using science mode, consider using a <a class="reference internal" href="#performance-workflow"><span class="std std-ref">workflow</span></a></p></li>
<li><p>Copy one of the job script examples within <code class="docutils literal notranslate"><span class="pre">MARVELsim/examples/HPC</span></code></p></li>
<li><p>Adjust the job script details: resources, paths/names, input parameters, etc.</p></li>
<li><p>Adjust the paths <code class="docutils literal notranslate"><span class="pre">POETRY</span></code>, <code class="docutils literal notranslate"><span class="pre">SIMDIR</span></code>, and <code class="docutils literal notranslate"><span class="pre">OUTDIR</span></code></p></li>
</ul>
</div>
<hr><div class="section" id="vsc-information">
<h2>VSC information<a class="headerlink" href="#vsc-information" title="Permalink to this headline">¶</a></h2>
<p>Since MARVELsim is being developed by the KU Leuven team we here provide further information for future users with access to the <a class="reference external" href="https://www.vscentrum.be/getaccess">Vlaams Supercomputer Centrum (VSC)</a>:</p>
<p>To get started using the VSC infrastrutrue we recommend reading:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://vlaams-supercomputing-centrum-vscdocumentation.readthedocs-hosted.com/en/latest/leuven/genius_quick_start.html#submit-to-genius-gpu-node">Genius quickstart guide</a></p></li>
<li><p><a class="reference external" href="https://vlaams-supercomputing-centrum-vscdocumentation.readthedocs-hosted.com/en/latest/leuven/tier2_hardware/genius_hardware.html">Genius hardware</a></p></li>
</ul>
</div></blockquote>
<p>For the VSC (Genius), further notice should be taken to:</p>
<blockquote>
<div><ul class="simple">
<li><p>Notice that adding more GPU nodes will not speed up the computations, however, some cluster do provide more GPUs which will decrease the run time.</p></li>
<li><p>We recommend to debug and test the computational resources needed for your jobs adding <code class="docutils literal notranslate"><span class="pre">#PBS</span> <span class="pre">-l</span> <span class="pre">qos=debugging</span></code> to the PSB details in the scripts shown above.</p></li>
<li><p>We note that in the following examples, the resources w.r.t. skylake GPU nodes are the maximum and, hence, the computation times stated above using the VSC are at their minimum.</p></li>
</ul>
</div></blockquote>
<hr></div>
<div class="section" id="job-script-calibration-mode">
<span id="performance-calibs"></span><h2>Job script - Calibration mode<a class="headerlink" href="#job-script-calibration-mode" title="Permalink to this headline">¶</a></h2>
<p>The first example (<code class="docutils literal notranslate"><span class="pre">run_calibs.pbs</span></code>) shows a typical job script for running a full set of calibrated data:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#PBS -N output</span>
<span class="c1">#PBS -A &lt;account/project&gt;</span>
<span class="c1">#PBS -l nodes=1:ppn=36:gpus=4:skylake</span>
<span class="c1">#PBS -l partition=gpu</span>
<span class="c1">#PBS -l pmem=2gb</span>
<span class="c1">#PBS -l walltime=10:00:00</span>

<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>

<span class="nv">POETRY</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/poetry/virtualenvs/marvelsim-3qcQCF7a-py3.8/bin/activate
<span class="nb">export</span> POETRY
<span class="nv">SIMDIR</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/MARVELsim/marvelsim
<span class="nb">export</span> SIMDIR
<span class="nv">OUTDIR</span><span class="o">=</span>/scratch/leuven/341/vsc34166/marvelsim/output
<span class="nb">export</span> OUTDIR

<span class="c1"># Activate poetry shell</span>
<span class="nb">source</span> <span class="nv">$POETRY</span>

<span class="c1"># Run calibration mode</span>
python <span class="nv">$SIMDIR</span>/marvelsim.py --calibs --cuda --zip -o <span class="nv">$OUTDIR</span>
</pre></div>
</div>
<p>Similar to the example given in the <a class="reference internal" href="tutorial.html#tutorial-calibration"><span class="std std-ref">tutorial</span></a>, we here use the off-the-shelf methodology of producing a set of calibrated data by simply invoking the flag <code class="docutils literal notranslate"><span class="pre">--calibs</span></code>. Furthermore we activate the flag <code class="docutils literal notranslate"><span class="pre">--cuda</span></code> and request a single node with 4 GPUs each using 9 CPU slaves (hence 36 in total) to execute the job. We request 2 GB of memory RAM to be on the safe side since a single <span class="math notranslate nohighlight">\(10,560 \times 10,560 \, \text{pixel}\)</span> full frame image occupy 851 Mb. In order to activate your Poetry shell the absolute path needs to be exported globally.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">poetry</span> <span class="pre">shell</span></code> is activated you can simply type <code class="docutils literal notranslate"><span class="pre">which</span> <span class="pre">python</span></code> to get the absolute path needed to add to the above variable <code class="docutils literal notranslate"><span class="pre">POETRY</span></code>. We here save the output data to the <strong>scratch</strong> file location in order avoid overflowing our memory storage on the <strong>data</strong> storage. Notice that it is possible to compress each image on the fly by enabling the flag <code class="docutils literal notranslate"><span class="pre">--zip</span></code> as done in this example. Typical deflation rates per image are around 80%, hence, it is highly recommended to invoke this flag for faster data transfer after end job. For the job script shown above the total run time (a.k.a. walltime) was 2 hours and 40 minutes.</p>
<hr></div>
<div class="section" id="job-script-science-mode">
<span id="performance-science"></span><h2>Job script - Science mode<a class="headerlink" href="#job-script-science-mode" title="Permalink to this headline">¶</a></h2>
<p>The following example (<code class="docutils literal notranslate"><span class="pre">run_science.pbs</span></code>) shows a job script for running 300 stellar spectra using a generated RV time series called <code class="docutils literal notranslate"><span class="pre">rv_data.txt</span></code>:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#PBS -N output</span>
<span class="c1">#PBS -A &lt;account_name&gt;</span>
<span class="c1">#PBS -l nodes=1:ppn=36:gpus=4:skylake</span>
<span class="c1">#PBS -l partition=gpu</span>
<span class="c1">#PBS -l pmem=2gb</span>
<span class="c1">#PBS -l walltime=12:00:00</span>

<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>

<span class="nv">POETRY</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/poetry/virtualenvs/marvelsim-3qcQCF7a-py3.8/bin/activate
<span class="nb">export</span> POETRY
<span class="nv">SIMDIR</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/MARVELsim/marvelsim
<span class="nb">export</span> SIMDIR
<span class="nv">OUTDIR</span><span class="o">=</span>/scratch/leuven/341/vsc34166/marvelsim/output
<span class="nb">export</span> OUTDIR

<span class="c1"># Activate poetry shell</span>
<span class="nb">source</span> <span class="nv">$POETRY</span>

<span class="c1"># Run science mode</span>
python <span class="nv">$SIMDIR</span>/marvelsim.py --science --time <span class="m">900</span> --mag <span class="m">10</span>.0 --teff <span class="m">5800</span> --logg <span class="m">4</span>.5 --z <span class="m">0</span>.0 --alpha <span class="m">0</span>.0 --data rv_data.txt --cuda --zip -o <span class="nv">$OUTPUT</span>
</pre></div>
</div>
<p>Akin to the previous job script we here use the same computational resources, however, with the exception of increasing the walltime and the flag <code class="docutils literal notranslate"><span class="pre">--science</span></code>. Notice that adding more nodes will not speed up the computations, however, some cluster do provide more GPUs which will decrease the run time. We recommend to debug and test the computational resources needed for your jobs adding <code class="docutils literal notranslate"><span class="pre">#PBS</span> <span class="pre">-l</span> <span class="pre">qos=debugging</span></code> to the PSB details in the scripts shown above and run a single simulation. We only use 6 CPUs since Pyxel needs a very large amount of RAM memory for each image (of the order of 25 Gb), hence, using only 1 node we are limited here to 6 CPUs in order not to overflow the nodes RAM memory. For the job script show above the total run time (walltime) was 10 hours. We further remark that Pyxel only needs the exposure time to apply CCD effects correctly which explains the absence of the stellar parameters. As shown from the workflow script above we used the popular <em>worker</em> framework to parallelise our simulations. Worker can immediately recognize the indices given in the first column of the RV data file <code class="docutils literal notranslate"><span class="pre">rv_data.txt</span></code> and used the <code class="docutils literal notranslate"><span class="pre">$index</span></code> parametrisation to automatically deligate the work to multiple CPU slaves.</p>
<hr></div>
<div class="section" id="workflow-science-mode">
<span id="performance-workflow"></span><h2>Workflow - Science mode<a class="headerlink" href="#workflow-science-mode" title="Permalink to this headline">¶</a></h2>
<p>If available, PyEchelle is extremely efficient to run with CUDA on NVIDIA hardware which typically is available for GPU nodes on most computing clusters. On the other hand Pyxel is not developed for the usage of GPUs but rather for normal CPU parallelisation. Thus, to not waste unesseary computional resources, we will in the following show how to run a so-called <em>workflow</em>; that is, to summit a combined script that first runs software 1 (i.e. PyEchelle on GPUs), and only when this finish succesfully, then run software 2 (i.e. Pyxel on CPUs) that has a input dependency from software 1 (i.e. the CCD full-frame spectra). For your conveniece we provide a ready-to-go script (<code class="docutils literal notranslate"><span class="pre">worflow_science.sh</span></code>) to be executed on the VSC:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1"># First summit PyEchlle job</span>
<span class="nv">workflow</span><span class="o">=</span><span class="k">$(</span>qsub run_science_pyechelle.pbs<span class="k">)</span>

<span class="c1"># When finished successfully summit Pyxel job</span>
wsub -W <span class="nv">depend</span><span class="o">=</span>afterok:<span class="nv">$workflow</span> -master -batch run_science_pyxel.pbs -data rv_data.txt
</pre></div>
</div>
<p>Like before we here used the standard Torque schedular command <code class="docutils literal notranslate"><span class="pre">qsub</span></code> to summit the PyEchelle job. The Pyxel job is submitted using the popular <code class="docutils literal notranslate"><span class="pre">worker</span></code> framework. By default worker use one node-core to schedule the simulation, however, as we only have a smaller amount of jobs (300 in total) we can overwrite this behavior and tell worker to use all node-cores for the computation. This is simply done by using the flag <code class="docutils literal notranslate"><span class="pre">-master</span></code>. Worker will automatically parameterise the <code class="docutils literal notranslate"><span class="pre">rv_data.txt</span></code> file for which we use the index and the RV amplitude from (see the output of the <span class="xref std std-ref">RV generator &lt;tutorial_rv_script</span>).</p>
<p>Currently, we only provide a workflow script (<code class="docutils literal notranslate"><span class="pre">examples/HPC/workflow_science.sh</span></code>) for the science mode. The important details here are the two job scripts called <code class="docutils literal notranslate"><span class="pre">run_science_pyechelle.pbs</span></code> and <code class="docutils literal notranslate"><span class="pre">run_science_pyxel.pbs</span></code> which each will invoke MARVELsim to run each software individually. We explain the details of these in the follwoing.</p>
<p>The following example (<code class="docutils literal notranslate"><span class="pre">run_science_pyechelle.pbs</span></code>) shows a job script for running 300 stellar spectra using a generated RV time series called <code class="docutils literal notranslate"><span class="pre">rv_data.txt</span></code>:</p>
<p>Compared to the science mode we haven’t made an effort to split up the computation between previous job scripts we here use the same computational resources</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#PBS -N output</span>
<span class="c1">#PBS -A &lt;account/project&gt;</span>
<span class="c1">#PBS -l nodes=1:ppn=36:gpus=4:skylake</span>
<span class="c1">#PBS -l partition=gpu</span>
<span class="c1">#PBS -l pmem=2gb</span>
<span class="c1">#PBS -l walltime=10:00:00</span>

<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>

<span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/MARVELsim/marvelsim/bin/python
<span class="nb">export</span> PYTHONPATH
<span class="nv">SIMDIR</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/MARVELsim
<span class="nb">export</span> SIMDIR

<span class="c1"># Activate environment</span>
<span class="nb">source</span> marvelsim/bin/activate

<span class="c1"># Run MARVELsim for PyEchelle only</span>
<span class="nb">cd</span> <span class="nv">$SIMDIR</span>
python simulator-marvel.py --time <span class="m">300</span> --mag <span class="m">10</span>.0 --teff <span class="m">5800</span> --logg <span class="m">4</span>.5 --z <span class="m">0</span>.0 --alpha <span class="m">0</span>.0 --data rv_data.txt --cuda -o <span class="nv">$SIMDIR</span>/output
</pre></div>
</div>
<p>Illustrated here we request a single node with 4 GPUs using each using 9 CPU claves (hence 36 in total) to execute the job. We request 2 GB of memory RAM to be on the safe side since a single 10,560 x 10,560 pixel full frame image occupy 851 Mb. The the run time (a.k.a. walltime) has here been timed to be around 10 hours.</p>
<p>Next we call MARVELsim to invoke Pyxel only using the job script (<code class="docutils literal notranslate"><span class="pre">run_science_pyxel.pbs</span></code>):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#PBS -N output</span>
<span class="c1">#PBS -A &lt;account&gt;</span>
<span class="c1">#PBS -l nodes=1:ppn=6:skylake</span>
<span class="c1">#PBS -l pmem=30gb</span>
<span class="c1">#PBS -l walltime=04:00:00</span>

<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>

<span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/MARVELsim/marvelsim/bin/python
<span class="nb">export</span> PYTHONPATH
<span class="nv">SIMDIR</span><span class="o">=</span><span class="nv">$VSC_DATA</span>/MARVELsim
<span class="nb">export</span> SIMDIR

<span class="c1"># Make sure to activate environment</span>
<span class="nb">source</span> marvelsim/bin/activate

<span class="c1"># Run star spectrum</span>
<span class="nb">cd</span> <span class="nv">$SIMDIR</span>
python simulator-marvel.py --time <span class="m">900</span> --dex <span class="nv">$index</span> --zip -o <span class="nv">$SIMDIR</span>/output
</pre></div>
</div>
<p>Seen here we only use 6 CPUs since Pyxel needs a very large amount of RAM memory for each image (of the order of 25 Gb), hence, using only 1 node we are limited here to 6 CPUs in order not to overflow the node memory. Notice that it is possible to compress each image on the fly by enabling the flag <code class="docutils literal notranslate"><span class="pre">zip</span></code> as done in this example. Typical deflation rates per image are around 80%, hence, it is highly recommended to invoke this flag for faster data transfer after end job. For the job script show above the total run time (walltime) was 3 hours. We further remark that Pyxel only needs the exposure time to apply CCD effects correctly which explains the absence of the stellar parameters. As shown from the workflow script above we used the popular <em>worker</em> framework to parallelise our simulations. Worker can immediately recognize the indices given in the first column of the RV data file <code class="docutils literal notranslate"><span class="pre">rv_data.txt</span></code> and used the <code class="docutils literal notranslate"><span class="pre">$index</span></code> parametrisation to automatically deligate the work to multiple CPU slaves.</p>
</div></blockquote>
</div>
<div class="section" id="workflows">
<h2>Workflows<a class="headerlink" href="#workflows" title="Permalink to this headline">¶</a></h2>
<p>If available PyEchelle is extremely efficient to run with CUDA on NVIDIA hardware which typically is available for GPU nodes on most computing clusters. On the other hand Pyxel is not developed for the usage of GPUs but rather for normal CPU prallelisation. Thus, to not waste unesseary computional resources, we will in the following show how to run a so-called <em>workflow</em>; that is, to summit a combined script that first runs software 1 (i.e. PyEchelle on GPUs), and only when this finish succesfully, then run software 2 (i.e. Pyxel on CPUs) that has a input dependency from software 1 (i.e. the CCD full-frame images). We conveniece we provide a ready-to-go script to be executed on the VSC:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1"># Clean and load modules</span>
module purge
module restore plato
module load worker

<span class="c1"># Summit jobs as a workflow</span>
<span class="nv">workflow1</span><span class="o">=</span><span class="k">$(</span>qsub run_science_pyechelle.pbs<span class="k">)</span>
wsub -W <span class="nv">depend</span><span class="o">=</span>afterok:<span class="nv">$workflow1</span> -batch run_science_pyxel.pbs -data data_200kms.txt
</pre></div>
</div>
<p>Currently, we only provide a workflow script (<code class="docutils literal notranslate"><span class="pre">MARVELsim/hpc/workflow_science.sh</span></code>) for the science mode. The important details here are the two job scripts called <code class="docutils literal notranslate"><span class="pre">run_science_pyechelle.pbs</span></code> and <code class="docutils literal notranslate"><span class="pre">run_science_pyxel.pbs</span></code> which each will invoke MARVELsim to run each software individually. We explain the details of these in the follwoing.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="extra.html" class="btn btn-neutral float-right" title="Extra examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="tutorial.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, KU Leuven.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>